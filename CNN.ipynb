{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d943b0b-cf3a-42c0-96b5-bcb255886e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6735023-c16a-4c53-9719-699451ebe57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images in brown folder:\n",
      "\n",
      "Processing images in green folder:\n",
      "\n",
      "Processing images in blue folder:\n",
      "\n",
      "Processing images in silver folder:\n",
      "\n",
      "Processing images in grey folder:\n",
      "\n",
      "Processing images in pink folder:\n",
      "\n",
      "Processing images in red folder:\n",
      "\n",
      "Processing images in gold folder:\n",
      "\n",
      "Processing images in purple folder:\n",
      "\n",
      "Processing images in beige folder:\n",
      "\n",
      "Processing images in yellow folder:\n",
      "\n",
      "Processing images in white folder:\n",
      "\n",
      "Processing images in black folder:\n",
      "\n",
      "Processing images in tan folder:\n",
      "\n",
      "Processing images in orange folder:\n",
      "\n",
      "Processing images in brown folder:\n",
      "\n",
      "Processing images in green folder:\n",
      "\n",
      "Processing images in blue folder:\n",
      "\n",
      "Processing images in silver folder:\n",
      "\n",
      "Processing images in grey folder:\n",
      "\n",
      "Processing images in pink folder:\n",
      "\n",
      "Processing images in red folder:\n",
      "\n",
      "Processing images in gold folder:\n",
      "\n",
      "Processing images in purple folder:\n",
      "\n",
      "Processing images in beige folder:\n",
      "\n",
      "Processing images in yellow folder:\n",
      "\n",
      "Processing images in white folder:\n",
      "\n",
      "Processing images in black folder:\n",
      "\n",
      "Processing images in tan folder:\n",
      "\n",
      "Processing images in orange folder:\n",
      "CPU times: user 4min, sys: 2min 1s, total: 6min 1s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def load_data(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    dominant_colors = []\n",
    "\n",
    "    for color_folder in os.listdir(folder_path):\n",
    "        color_path = os.path.join(folder_path, color_folder)\n",
    "        if os.path.isdir(color_path):\n",
    "            print(f\"\\nProcessing images in {color_folder} folder:\")\n",
    "\n",
    "\n",
    "            for image_file in os.listdir(color_path):\n",
    "                image_path = os.path.join(color_path, image_file)\n",
    "                if image_file.lower() == '.ds_store':\n",
    "                    print(f\"Skipping {image_path}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    valid_extensions = ['.jpg', '.jpeg', '.png']\n",
    "                    if os.path.isfile(image_path) and any(image_path.lower().endswith(ext) for ext in valid_extensions):\n",
    "                        image = io.imread(image_path)\n",
    "                        images.append(transform.resize(image, (100, 100)))  \n",
    "                        labels.append(color_folder)\n",
    "\n",
    "                      \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "train_folder_path = '/Users/tolga/Desktop/CS464/archive/train'\n",
    "validation_folder_path = '/Users/tolga/Desktop/CS464/archive/val'\n",
    "\n",
    "train_images, train_labels = load_data(train_folder_path)\n",
    "validation_images, validation_labels = load_data(validation_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b73403-2b1e-4e2a-87c1-327d3958c82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 4.48 s, total: 6.3 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "images = np.array(train_images)  \n",
    "labels = np.array(train_labels)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "images_val=np.array(validation_images)\n",
    "labels_val=np.array(validation_labels)\n",
    "labels_val=label_encoder.fit_transform(labels_val)\n",
    "\n",
    "val_inputs_t = torch.tensor(images_val, dtype=torch.float32)\n",
    "val_inputs_tensor =val_inputs_t.permute(0, 3, 1, 2)\n",
    "val_labels_tensor = torch.tensor(labels_val, dtype=torch.long)\n",
    "\n",
    "dataset_val = TensorDataset(val_inputs_tensor, val_labels_tensor)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1550 ,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "images_t = torch.tensor(images, dtype=torch.float32)\n",
    "images_tensor= images_t.permute(0, 3, 1, 2)\n",
    "labels_tensor = torch.tensor(labels_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d236f4c-46be-4576-9f10-a762a8d9a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(6) # Batch normalization after conv1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=10, kernel_size=5, stride=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(10) \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=32, kernel_size=3, stride=1)  # New conv layer\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "       \n",
    "        self.fc1 = nn.Linear(32 * 10 * 10, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)  #dropout probability of 0.5\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, len(label_encoder.classes_)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.batch_norm1(nn.functional.relu(self.conv1(x))))\n",
    "        x = self.pool2(self.batch_norm2(nn.functional.relu(self.conv2(x))))\n",
    "        x = self.pool3(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32 * 10 * 10)  # this flattens the tensor\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Applying dropout after the first fully connected layer\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = CustomCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec67401b-b7c8-473b-b5b6-ddf137b9a836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [50/73], Loss: 2.1656\n",
      "Epoch [2/15], Step [50/73], Loss: 1.4506\n",
      "Epoch [3/15], Step [50/73], Loss: 1.2138\n",
      "Epoch [4/15], Step [50/73], Loss: 1.0870\n",
      "Epoch [5/15], Step [50/73], Loss: 0.9803\n",
      "Epoch [6/15], Step [50/73], Loss: 0.9425\n",
      "Epoch [7/15], Step [50/73], Loss: 0.9048\n",
      "Epoch [8/15], Step [50/73], Loss: 0.8764\n",
      "Epoch [9/15], Step [50/73], Loss: 0.8139\n",
      "Epoch [10/15], Step [50/73], Loss: 0.7688\n",
      "Epoch [11/15], Step [50/73], Loss: 0.7883\n",
      "Epoch [12/15], Step [50/73], Loss: 0.7289\n",
      "Epoch [13/15], Step [50/73], Loss: 0.7486\n",
      "Epoch [14/15], Step [50/73], Loss: 0.6999\n",
      "Epoch [15/15], Step [50/73], Loss: 0.7115\n",
      "Finished Training\n",
      "CPU times: user 16min 27s, sys: 5min 58s, total: 22min 26s\n",
      "Wall time: 3min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:  # print every 100 mini batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                  f\"Step [{i + 1}/{len(dataloader)}], \"\n",
    "                  f\"Loss: {running_loss / 50:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c2b19c-d03f-4c50-8ecd-018068aa91b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 80.40%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloader_val:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {100 * accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd32a6e-2cc1-4656-8872-903b41c3f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in dataloader_val:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "plt.savefig(' conf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2d2d8-27d7-4d1a-a489-524daffa6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), '/Users/tolga/Desktop/CS464/Project/model_82_1.pth')\n",
    "\n",
    "\n",
    "#best model optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001) batch size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fa6c6-3efb-4b73-99df-1e9d8fa0e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "accuracy = correct / total\n",
    "print(f'Train Accuracy: {100 * accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c1a3c-98f2-4b59-9e5b-78c0a660839d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47906041-e29d-4585-a590-a02552face84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through the validation DataLoader\n",
    "with torch.no_grad():\n",
    "    for data in dataloader_val:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Convert PyTorch tensors to numpy arrays\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885f00f-0cc5-44ff-9fb8-9253554b2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(conf_matrix, annot=True, fmt='d', )\n",
    "ax.set_xlabel(\"Predicted Label\")\n",
    "ax.set_ylabel(\"Actual Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478aa39-b02d-4d65-b877-49674792b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datloader_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb1daa-36ab-4e4d-9c6d-a95fa5a58525",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb6157-2b04-421a-af7a-3db3afda979a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
